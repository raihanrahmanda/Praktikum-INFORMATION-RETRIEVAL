{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "def termFrequencyInDoc(vocab, doc_dict):\n",
    "    tf_docs = {}\n",
    "    for doc_id in doc_dict.keys():\n",
    "        tf_docs[doc_id] = {}\n",
    "    for word in vocab:\n",
    "        for doc_id,doc in doc_dict.items():\n",
    "            tf_docs[doc_id][word] = doc.count(word)\n",
    "    return tf_docs\n",
    "\n",
    "def tokenisasi(text):\n",
    "    tokens = text.split(\" \")\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc1_term = [\"pengembangan\", \"sistem\", \"informasi\", \"penjadwalan\"]\n",
    "doc2_term = [\"pengembangan\", \"model\", \"analisis\", \"sentimen\", \"berita\"]\n",
    "doc3_term = [\"analisis\", \"sistem\", \"input\", \"output\"]\n",
    "corpus_term = [doc1_term, doc2_term, doc3_term]\n",
    "\n",
    "inverted_index = {}\n",
    "for i in range(len(corpus_term)):\n",
    "    for item in corpus_term[i]:\n",
    "        stemmer = StemmerFactory().create_stemmer()\n",
    "        item = stemmer.stem(item)\n",
    "        if item not in inverted_index:\n",
    "            inverted_index[item] = []\n",
    "        if item in inverted_index:\n",
    "            inverted_index[item].append(i+1)\n",
    "\n",
    "vocab=list(inverted_index.keys())\n",
    "doc_dict = {}\n",
    "\n",
    "#clean after stemming\n",
    "doc_dict['doc1'] = \"kembang sistem informasi jadwal\"\n",
    "doc_dict['doc2'] = \"kembang model analisis sentimen berita\"\n",
    "doc_dict['doc3'] = \"analisis sistem input output\"\n",
    "\n",
    "print(termFrequencyInDoc(vocab, doc_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kembang': 1.2876820724517808, 'sistem': 1.2876820724517808, 'informasi': 1.6931471805599454, 'jadwal': 1.6931471805599454, 'model': 1.6931471805599454, 'analisis': 1.2876820724517808, 'sentimen': 1.6931471805599454, 'berita': 1.6931471805599454, 'input': 1.6931471805599454, 'output': 1.6931471805599454}\n"
     ]
    }
   ],
   "source": [
    "def wordDocFre(vocab, doc_dict):\n",
    "    df = {}\n",
    "    for word in vocab:\n",
    "        frq = 0\n",
    "        for doc in doc_dict.values():\n",
    "            if word in tokenisasi(doc):\n",
    "                frq = frq + 1\n",
    "        df[word] = frq\n",
    "    return df\n",
    "\n",
    "import numpy as np\n",
    "def inverseDocFre(vocab,doc_fre,length):\n",
    "    idf= {}\n",
    "    for word in vocab:\n",
    "        idf[word] = idf[word] = 1 + np.log((length + 1) / (doc_fre[word]+1))\n",
    "    return idf\n",
    "\n",
    "print(inverseDocFre(vocab, wordDocFre(vocab, doc_dict), len(doc_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.28768207 1.28768207 0.        ]\n",
      " [1.28768207 0.         1.28768207]\n",
      " [1.69314718 0.         0.        ]\n",
      " [1.69314718 0.         0.        ]\n",
      " [0.         1.69314718 0.        ]\n",
      " [0.         1.28768207 1.28768207]\n",
      " [0.         1.69314718 0.        ]\n",
      " [0.         1.69314718 0.        ]\n",
      " [0.         0.         1.69314718]\n",
      " [0.         0.         1.69314718]]\n"
     ]
    }
   ],
   "source": [
    "def tfidf(vocab,tf,idf_scr,doc_dict):\n",
    "    tf_idf_scr = {}\n",
    "    for doc_id in doc_dict.keys():\n",
    "        tf_idf_scr[doc_id] = {}\n",
    "    for word in vocab:\n",
    "        for doc_id,doc in doc_dict.items():\n",
    "            tf_idf_scr[doc_id][word] = tf[doc_id][word] * idf_scr[word]\n",
    "    return tf_idf_scr\n",
    "\n",
    "tf_idf = tfidf(vocab, termFrequencyInDoc(vocab, doc_dict), inverseDocFre(vocab, wordDocFre(vocab, doc_dict), len(doc_dict)), doc_dict)\n",
    "# Term - Document Matrix\n",
    "TD = np.zeros((len(vocab), len(doc_dict)))\n",
    "for word in vocab:\n",
    "    for doc_id,doc in tf_idf.items():\n",
    "        ind1 = vocab.index(word)\n",
    "        ind2 = list(tf_idf.keys()).index(doc_id)\n",
    "        TD[ind1][ind2] = tf_idf[doc_id][word]\n",
    "print(TD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "def edit_distance(string1, string2):\n",
    "    if len(string1) > len(string2):\n",
    "        difference = len(string1) - len(string2)\n",
    "        string1[:difference]\n",
    "        n = len(string2)\n",
    "    elif len(string2) > len(string1):\n",
    "        difference = len(string2) - len(string1)\n",
    "        string2[:difference]\n",
    "        n = len(string1)\n",
    "    for i in range(n):\n",
    "        if string1[i] != string2[i]:\n",
    "            difference += 1\n",
    "\n",
    "    return difference\n",
    "\n",
    "print(edit_distance(doc_dict['doc1'], doc_dict['doc2']))\n",
    "print(edit_distance(doc_dict['doc1'], doc_dict['doc3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n",
      "0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "def jaccard_sim(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "\n",
    "    return float(intersection) / union\n",
    "\n",
    "print(jaccard_sim(doc_dict['doc1'].split(\" \"), doc_dict['doc2'].split(\" \")))\n",
    "print(jaccard_sim(doc_dict['doc1'].split(\" \"), doc_dict['doc3'].split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.201188773980276\n",
      "3.844897884155026\n"
     ]
    }
   ],
   "source": [
    "def euclidian_dist(vec1, vec2):\n",
    "    # subtracting vector\n",
    "    temp = vec1 - vec2\n",
    "    # doing dot product\n",
    "    # for finding\n",
    "    # sum of the squares\n",
    "    sum_sq = np.dot(temp.T, temp)\n",
    "    # Doing squareroot and\n",
    "    # printing Euclidean distance\n",
    "    return np.sqrt(sum_sq)\n",
    "\n",
    "print(euclidian_dist(TD[:, 0], TD[:, 1])) #doc1 & doc2\n",
    "print(euclidian_dist(TD[:, 0], TD[:, 2])) #doc1 & doc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15967058203849993\n",
      "0.1832234081332565\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def cosine_sim(vec1, vec2):\n",
    "    vec1 = list(vec1)\n",
    "    vec2 = list(vec2)\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "\n",
    "    return dot_prod / (mag_1 * mag_2)\n",
    "\n",
    "print(cosine_sim(TD[:, 0], TD[:, 1])) #doc1 & doc2\n",
    "print(cosine_sim(TD[:, 0], TD[:, 2])) #doc1 & doc3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
