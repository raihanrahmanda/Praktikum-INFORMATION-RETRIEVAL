{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisasi(text):\n",
    "    tokens = text.split(\" \")\n",
    "    return tokens\n",
    "\n",
    "def stemming(text):\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    # create stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    # stemming process\n",
    "    output = stemmer.stem(text)\n",
    "    return output\n",
    "\n",
    "\n",
    "def stemming_sentence(text):\n",
    "    output = \"\"\n",
    "    for token in tokenisasi(text):\n",
    "        output = output + stemming(token) + \" \"\n",
    "    return output[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc1': 'kembang sistem informasi jadwal', 'doc2': 'kembang model analisis sentimen berita', 'doc3': 'analisis sistem input output', 'doc4': 'kembang sistem informasi akademik universitas', 'doc5': 'kembang sistem cari berita ekonomi', 'doc6': 'analisis sistem neraca nasional', 'doc7': 'kembang sistem informasi layan statistik', 'doc8': 'kembang sistem cari skripsi di universitas', 'doc9': 'analisis sentimen publik hadap perintah', 'doc10': 'kembang model klasifikasi sentimen berita'}\n"
     ]
    }
   ],
   "source": [
    "doc_dict_raw = {}\n",
    "doc_dict_raw['doc1'] = \"pengembangan sistem informasi penjadwalan\"\n",
    "doc_dict_raw['doc2'] = \"pengembangan model analisis sentimen berita\"\n",
    "doc_dict_raw['doc3'] = \"analisis sistem input output\"\n",
    "doc_dict_raw['doc4'] = \"pengembangan sistem informasi akademik universitas\"\n",
    "doc_dict_raw['doc5'] = \"pengembangan sistem cari berita ekonomi\"\n",
    "doc_dict_raw['doc6'] = \"analisis sistem neraca nasional\"\n",
    "doc_dict_raw['doc7'] = \"pengembangan sistem informasi layanan statistik\"\n",
    "doc_dict_raw['doc8'] = \"pengembangan sistem pencarian skripsi di universitas\"\n",
    "doc_dict_raw['doc9'] = \"analisis sentimen publik terhadap pemerintah\"\n",
    "doc_dict_raw['doc10'] = \"pengembangan model klasifikasi sentimen berita\"\n",
    "\n",
    "doc_dict = {}\n",
    "for doc_id,doc in doc_dict_raw.items():\n",
    "    doc_dict[doc_id] = stemming_sentence(doc)\n",
    "print(doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kembang\n",
      "sistem\n",
      "informasi\n",
      "jadwal\n",
      "kembang\n",
      "model\n",
      "analisis\n",
      "sentimen\n",
      "berita\n",
      "analisis\n",
      "sistem\n",
      "input\n",
      "output\n",
      "kembang\n",
      "sistem\n",
      "informasi\n",
      "akademik\n",
      "universitas\n",
      "kembang\n",
      "sistem\n",
      "cari\n",
      "berita\n",
      "ekonomi\n",
      "analisis\n",
      "sistem\n",
      "neraca\n",
      "nasional\n",
      "kembang\n",
      "sistem\n",
      "informasi\n",
      "layan\n",
      "statistik\n",
      "kembang\n",
      "sistem\n",
      "cari\n",
      "skripsi\n",
      "di\n",
      "universitas\n",
      "analisis\n",
      "sentimen\n",
      "publik\n",
      "hadap\n",
      "perintah\n",
      "kembang\n",
      "model\n",
      "klasifikasi\n",
      "sentimen\n",
      "berita\n",
      "['kembang', 'sistem', 'informasi', 'jadwal', 'model', 'analisis', 'sentimen', 'berita', 'input', 'output', 'akademik', 'universitas', 'cari', 'ekonomi', 'neraca', 'nasional', 'layan', 'statistik', 'skripsi', 'di', 'publik', 'hadap', 'perintah', 'klasifikasi']\n",
      "{'kembang': ['doc1', 'doc2', 'doc4', 'doc5', 'doc7', 'doc8', 'doc10'], 'sistem': ['doc1', 'doc3', 'doc4', 'doc5', 'doc6', 'doc7', 'doc8'], 'informasi': ['doc1', 'doc4', 'doc7'], 'jadwal': ['doc1'], 'model': ['doc2', 'doc10'], 'analisis': ['doc2', 'doc3', 'doc6', 'doc9'], 'sentimen': ['doc2', 'doc9', 'doc10'], 'berita': ['doc2', 'doc5', 'doc10'], 'input': ['doc3'], 'output': ['doc3'], 'akademik': ['doc4'], 'universitas': ['doc4', 'doc8'], 'cari': ['doc5', 'doc8'], 'ekonomi': ['doc5'], 'neraca': ['doc6'], 'nasional': ['doc6'], 'layan': ['doc7'], 'statistik': ['doc7'], 'skripsi': ['doc8'], 'di': ['doc8'], 'publik': ['doc9'], 'hadap': ['doc9'], 'perintah': ['doc9'], 'klasifikasi': ['doc10']}\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "inverted_index = {}\n",
    "for doc_id,doc in doc_dict.items():\n",
    "    for token in tokenisasi(doc):\n",
    "        print(token)\n",
    "        if token not in vocab:\n",
    "            vocab.append(token)\n",
    "            inverted_index[token] = []\n",
    "        if token in inverted_index:\n",
    "            if doc_id not in inverted_index[token]:\n",
    "                inverted_index[token].append(doc_id)\n",
    "print(vocab)\n",
    "print(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"sistem informasi statistik\"\n",
    "def termFrequency(vocab, query):\n",
    "    tf_query = {}\n",
    "    for word in vocab:\n",
    "        tf_query[word] = query.count(word)\n",
    "    return tf_query\n",
    "\n",
    "tf_query = termFrequency(vocab, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [1.31845373]\n",
      " [2.01160091]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.70474809]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Term - Query Matrix\n",
    "import numpy as np\n",
    "def wordDocFre(vocab, doc_dict):\n",
    "    df = {}\n",
    "    for word in vocab:\n",
    "        frq = 0\n",
    "        for doc in doc_dict.values():\n",
    "            if word in tokenisasi(doc):\n",
    "                frq = frq + 1\n",
    "        df[word] = frq\n",
    "    return df\n",
    "\n",
    "def inverseDocFre(vocab,doc_fre,length):\n",
    "    idf= {}\n",
    "    for word in vocab:\n",
    "        idf[word] = idf[word] = 1 + np.log((length + 1) / (doc_fre[word]+1))\n",
    "    return idf\n",
    "\n",
    "def termFrequencyInDoc(vocab, doc_dict):\n",
    "    tf_docs = {}\n",
    "    for doc_id in doc_dict.keys():\n",
    "        tf_docs[doc_id] = {}\n",
    "    for word in vocab:\n",
    "        for doc_id,doc in doc_dict.items():\n",
    "            tf_docs[doc_id][word] = doc.count(word)\n",
    "    return tf_docs\n",
    "\n",
    "idf = inverseDocFre(vocab, wordDocFre(vocab, doc_dict), len(doc_dict))\n",
    "\n",
    "TQ = np.zeros((len(vocab), 1)) #hanya 1 query\n",
    "for word in vocab:\n",
    "    ind1 = vocab.index(word)\n",
    "    TQ[ind1][0] = tf_query[word]*idf[word]\n",
    "print(TQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.31845373 1.31845373 0.         1.31845373 1.31845373 0.\n",
      "  1.31845373 1.31845373 0.         1.31845373]\n",
      " [1.31845373 0.         1.31845373 1.31845373 1.31845373 1.31845373\n",
      "  1.31845373 1.31845373 0.         0.        ]\n",
      " [2.01160091 0.         0.         2.01160091 0.         0.\n",
      "  2.01160091 0.         0.         0.        ]\n",
      " [2.70474809 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         2.29928298 0.         0.         0.         0.\n",
      "  0.         0.         0.         2.29928298]\n",
      " [0.         1.78845736 1.78845736 0.         0.         1.78845736\n",
      "  0.         0.         1.78845736 0.        ]\n",
      " [0.         2.01160091 0.         0.         0.         0.\n",
      "  0.         0.         2.01160091 2.01160091]\n",
      " [0.         2.01160091 0.         0.         2.01160091 0.\n",
      "  0.         0.         0.         2.01160091]\n",
      " [0.         0.         2.70474809 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         2.70474809 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         2.70474809 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         2.29928298 0.         0.\n",
      "  0.         2.29928298 0.         0.        ]\n",
      " [0.         0.         0.         0.         2.29928298 0.\n",
      "  0.         2.29928298 0.         0.        ]\n",
      " [0.         0.         0.         0.         2.70474809 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         2.70474809\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         2.70474809\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  2.70474809 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  2.70474809 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         2.70474809 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         2.70474809 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         2.70474809 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         2.70474809 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         2.70474809 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         2.70474809]]\n"
     ]
    }
   ],
   "source": [
    "def tfidf(vocab,tf,idf_scr,doc_dict):\n",
    "    tf_idf_scr = {}\n",
    "    for doc_id in doc_dict.keys():\n",
    "        tf_idf_scr[doc_id] = {}\n",
    "    for word in vocab:\n",
    "        for doc_id,doc in doc_dict.items():\n",
    "            tf_idf_scr[doc_id][word] = tf[doc_id][word] * idf_scr[word]\n",
    "    return tf_idf_scr\n",
    "\n",
    "tf_idf = tfidf(vocab, termFrequencyInDoc(vocab, doc_dict), inverseDocFre(vocab, wordDocFre(vocab, doc_dict), len(doc_dict)), doc_dict)\n",
    "# Term - Document Matrix\n",
    "TD = np.zeros((len(vocab), len(doc_dict)))\n",
    "for word in vocab:\n",
    "    for doc_id,doc in tf_idf.items():\n",
    "        ind1 = vocab.index(word)\n",
    "        ind2 = list(tf_idf.keys()).index(doc_id)\n",
    "        TD[ind1][ind2] = tf_idf[doc_id][word]\n",
    "print(TD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.414904809442661\n",
      "0.0\n",
      "0.10856998991379904\n",
      "0.35626622628022314\n",
      "0.10705617011820337\n",
      "0.10856998991379904\n",
      "0.7689768599816609\n",
      "0.08967792817935699\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def cosine_sim(vec1, vec2):\n",
    "    vec1 = list(vec1)\n",
    "    vec2 = list(vec2)\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "\n",
    "    return dot_prod / (mag_1 * mag_2)\n",
    "\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 0])) #query & doc1\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 1])) #query & doc2\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 2])) #query & doc3\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 3])) #query & doc4\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 4])) #query & doc5\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 5])) #query & doc6\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 6])) #query & doc7\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 7])) #query & doc8\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 8])) #query & doc9\n",
    "print(cosine_sim(TQ[:, 0], TD[:, 9])) #query & doc10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc7': 0.7689768599816609, 'doc1': 0.414904809442661, 'doc4': 0.35626622628022314}\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "def exact_top_k(doc_dict, TD, q, k):\n",
    "    relevance_scores = {}\n",
    "    i = 0\n",
    "    for doc_id in doc_dict.keys():\n",
    "        relevance_scores[doc_id] = cosine_sim(q, TD[:, i])\n",
    "        i = i + 1\n",
    "\n",
    "    sorted_value = OrderedDict(sorted(relevance_scores.items(), key=lambda x: x[1], reverse = True))\n",
    "    top_k = {j: sorted_value[j] for j in list(sorted_value)[:k]}\n",
    "    return top_k\n",
    "\n",
    "top_3 = exact_top_k(doc_dict, TD, TQ[:, 0], 3)\n",
    "print(top_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc1': 'kembang sistem informasi jadwal', 'doc2': 'kembang model analisis sentimen berita', 'doc3': 'analisis sistem input output', 'doc4': 'kembang sistem informasi akademik universitas', 'doc5': 'kembang sistem cari berita ekonomi', 'doc6': 'analisis sistem neraca nasional', 'doc7': 'kembang sistem informasi layan statistik', 'doc8': 'kembang sistem cari skripsi di universitas', 'doc9': 'analisis sentimen publik hadap perintah', 'doc10': 'kembang model klasifikasi sentimen berita'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc1': 'kembang sistem informasi jadwal', 'doc3': 'analisis sistem input output', 'doc4': 'kembang sistem informasi akademik universitas', 'doc5': 'kembang sistem cari berita ekonomi', 'doc6': 'analisis sistem neraca nasional', 'doc7': 'kembang sistem informasi layan statistik', 'doc8': 'kembang sistem cari skripsi di universitas'}\n"
     ]
    }
   ],
   "source": [
    "def index_elim_simple(query, doc_dict):\n",
    "    remove_list =[]\n",
    "    for doc_id,doc in doc_dict.items():\n",
    "        n = 0\n",
    "        for word in tokenisasi(query):\n",
    "            if stemming(word) in doc:\n",
    "                n = n+1\n",
    "        if n==0:\n",
    "            remove_list.append(doc_id)\n",
    "    for key in remove_list:\n",
    "        del doc_dict[key]\n",
    "    return doc_dict\n",
    "\n",
    "print(doc_dict)\n",
    "query = \"sistem informasi statistik\"\n",
    "doc_dict = index_elim_simple(query, doc_dict)\n",
    "print(doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "informasi statistik\n"
     ]
    }
   ],
   "source": [
    "def elim_query(query, idf_dict, idf_score):\n",
    "    for term in tokenisasi(query):\n",
    "        if idf_dict[stemming(term)]<idf_score:\n",
    "            query = query.replace(term+\" \", \"\")\n",
    "            query = query.replace(term, \"\")\n",
    "    return query\n",
    "\n",
    "query = \"sistem informasi statistik\"\n",
    "query = elim_query(query, idf, 1.5)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kembang': ['doc1', 'doc2'],\n",
       " 'sistem': ['doc1', 'doc3'],\n",
       " 'informasi': ['doc1', 'doc4'],\n",
       " 'jadwal': ['doc1'],\n",
       " 'model': ['doc2', 'doc10'],\n",
       " 'analisis': ['doc2', 'doc3'],\n",
       " 'sentimen': ['doc2', 'doc9'],\n",
       " 'berita': ['doc2', 'doc5'],\n",
       " 'input': ['doc3'],\n",
       " 'output': ['doc3'],\n",
       " 'akademik': ['doc4'],\n",
       " 'universitas': ['doc4', 'doc8'],\n",
       " 'cari': ['doc5', 'doc8'],\n",
       " 'ekonomi': ['doc5'],\n",
       " 'neraca': ['doc6'],\n",
       " 'nasional': ['doc6'],\n",
       " 'layan': ['doc7'],\n",
       " 'statistik': ['doc7'],\n",
       " 'skripsi': ['doc8'],\n",
       " 'di': ['doc8'],\n",
       " 'publik': ['doc9'],\n",
       " 'hadap': ['doc9'],\n",
       " 'perintah': ['doc9'],\n",
       " 'klasifikasi': ['doc10']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_championlist(inverted_index, tf_idf, r):\n",
    "    champion_list = {}\n",
    "    for term in inverted_index.keys():\n",
    "        weight_scores = {}\n",
    "        for doc_id,tf in tf_idf.items():\n",
    "            if tf_idf[doc_id][term]!=0:\n",
    "                weight_scores[doc_id] = tf_idf[doc_id][term]\n",
    "        sorted_value = OrderedDict(sorted(weight_scores.items(), key=lambda x: x[1], reverse = True))\n",
    "        top_r = {j: sorted_value[j] for j in list(sorted_value)[:r]}\n",
    "        champion_list[term]=list(top_r.keys())\n",
    "    return champion_list\n",
    "\n",
    "r=2\n",
    "create_championlist(inverted_index, tf_idf, r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
